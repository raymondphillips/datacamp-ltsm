{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Tutorial\n",
    "\n",
    "This tutorial will guid you on how to use the Tensorboard. Tensorboard is an amazing utility that allows us to visualize data and how it behaves. In this tutorial, you will see for what sort of purposes you can use the Tensorboard when training a neural network. \n",
    "\n",
    "First you will be explained how to start the Tensorboard, followed by an enlisting of the different views offered in the Tesorboard. Next you can learn how you can visualize scalar values produced during computations, on the Tensorboard. You will also see how this provide insights to the model to fix any potential errors in the learning. Thereafter you will investigate how you can visualize vectors or collections of data as histograms using the Tensorboard. With this view you will compare how weight initialization of the neural network affects the weight update of the neural network during the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thushan\\documents\\python_virtualenvs\\tensorflow_venv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Make sure that you have all these libaries available to run the code successfully\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json \n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Tensorboard\n",
    "To visualize things via Tensorboard, you first need to start the Tensorboard service. For that,\n",
    "\n",
    "1. Open up the command prompt (Windows) or a terminal (Ubuntu/Mac)\n",
    "2. Go into the project home directory\n",
    "3. If you are using python virtuanenv, activate the virtual environment you have installed TensorFlow\n",
    "4.  Make sure you can see the TensorFlow library through Python. For that,\n",
    " * Type in `python3`, you will get a \">>>\" looking prompt\n",
    " * Try `import tensorflow as tf`\n",
    " * If you can run this successfully you are fine\n",
    "5. Exit the python prompt (that is, \">>>\") by typing `exit()` and type in the following command\n",
    " * `tensorboard --logdir=summaries`\n",
    " * `--logdir` is the directory you will create data to visualize\n",
    " * Files that Tensorboard save data into are called *event files*\n",
    " * Type of data saved into the event files is called *summary data*\n",
    " * Optionally you can use `--port=<port_you_like>` to change the port Tensorboard runs on \n",
    "6. You should now get the following message\n",
    " * TensorBoard 1.6.0 at &lt;url&gt;:6006 (Press CTRL+C to quit)\n",
    "7. Enter the &lt;url&gt;:6006 in to the web browser\n",
    " * You should be able to see a orange dashboard at this point. You won't have anything to display because you haven't generated data.\n",
    "\n",
    "**Note**: Tensorboard does not like to see multiple event files in the same directory. This can lead to you getting very gruesome curves on the display. So for each different example you create a separate folder (e.g. summaries/first, summaries/second, ...) to save data. Another thing to keep in mind is that, if you want to re-run an experiment (that is, saving event file to an already populated folder), you have to make sure to first delete the existing event files.\n",
    "\n",
    "## Different Views of Tensorboard\n",
    "\n",
    "Different views take inputs of different formats and display them differently. You can change the view on the top orange bar on the Tensorboard\n",
    "* **Scalars** - Visualize scalar values (e.g. classification accuracy)\n",
    "* **Graph** - Visualize the computational graph of your model (e.g. neural network model)\n",
    "* **Distributions** - Visualize how data changes over time (e.g. weights of a neural network)\n",
    "* **Histograms** - A fancier view of the distribution that shows distributions in a 3-dimensional perspective\n",
    "* Projector - Can be used to visualize word embeddings (that is, word embeddings are numerical representations of words that capture their semantic relationships)\n",
    "* Image - Visualizing image data\n",
    "* Audio - Visualizing audio data\n",
    "* Text - Visualizing text (string) data\n",
    "\n",
    "In this tutorial you will cover the views shown in bold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Benefit of Scalar Visualization through Tensorboard\n",
    "\n",
    "In this section, you will first understand why visualizing certain things (e.g. loss or accuracy) is beneficial. When training deep neural networks, one of the crucial issues that strikes the beginners is the lack of understanding the effects of various design choices and hyperparameters. For example, if you carelessly initialize weights of a deep neural network to have a very large variance between weights, your model will quickly diverge and collapse. On the other hand, things can go wrong even when you are quite competent in taming neural networks to make use of them. For example, not paying attention to the learning rate can lead to either the divergence of the model or pre-maturely saturating to sub-optimal performance. \n",
    "\n",
    "One way quickly detect problems with your model is to have a graphical visualization of what's going on in our model in real time (for example, every 100 iterations). So if your model is behaving oddly, it will be clearly visible. That is exactly what tensorboard provides you with. You can decide which values needs to be displayed on the Tensorboard and Tensorboard will keep maintain a real time visualization of those values during learning.\n",
    "\n",
    "You start by first creating a five-layer neural network that you will use to classify hand-written digit images. For that you will use the famous MNIST dataset. TensorFlow provides a simple API to load MNIST data, so you don't have to manually download data. Before that you define a simple method (that is, `accuracy()`) that calculates the accuracy of some predictions with respect to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions,labels):\n",
    "    '''\n",
    "    Accuracy of a given set of predictions of size (N x n_classes) and\n",
    "    labels of size (N x n_classes)\n",
    "    '''\n",
    "    return np.sum(np.argmax(predictions,axis=1)==np.argmax(labels,axis=1))*100.0/labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Inputs, Outputs, Weights and Biases\n",
    "\n",
    "First you define a `batch_size` denoting the amount of data you sample at a single optimization/validation or testing step. Then you define the `layer_ids`, which gives an identifier for each of the layers of the neural network you will be defining. You then can define `layer_sizes`. Note that `len(layer_sizes)` should be `len(layer_ids)+1`, because `layer_sizes` includes the size of the input at the beginning. MNIST has images of size 28x28, which will be 784 when unwrapped to a single dimension. Then you can define the input and label placeholders, that you will later use to train the model. Finally you define two TensorFlow variables for each layer (that is, `weights` and `bias`).\n",
    "\n",
    "You can use variable scoping (more information [here](https://www.tensorflow.org/programmers_guide/variables)) so that the variables will be nicely named and will be much easier to access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "layer_ids = ['hidden1','hidden2','hidden3','hidden4','hidden5','out']\n",
    "layer_sizes = [784, 500, 400, 300, 200, 100, 10]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs and Labels\n",
    "train_inputs = tf.placeholder(tf.float32, shape=[batch_size, layer_sizes[0]], name='train_inputs')\n",
    "train_labels = tf.placeholder(tf.float32, shape=[batch_size, layer_sizes[-1]], name='train_labels')\n",
    "\n",
    "# Weight and Bias definitions\n",
    "for idx, lid in enumerate(layer_ids):\n",
    "    \n",
    "    with tf.variable_scope(lid):\n",
    "        w = tf.get_variable('weights',shape=[layer_sizes[idx], layer_sizes[idx+1]], \n",
    "                            initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "        b = tf.get_variable('bias',shape= [layer_sizes[idx+1]], \n",
    "                            initializer=tf.random_uniform_initializer(-0.1,0.1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Logits, Predictions, Loss and Optimization\n",
    "\n",
    "With the input/output placeholders, weights and biases of each layer defined, you now can define the calculations to calculate the logits of the neural network. Logits are the unnormalized values produced at the last layer of the neural network. When normalized, you call them predictions. This involves iterating through each layer in the neural network and computing `tf.matmul(h,w) +b`. You also need to apply an activation function as `tf.nn.relu(tf.matmul(h,w) +b)`, for all layers except for the last layer.\n",
    "\n",
    "Next you define loss function that is used to optimize the neural network. In this example, you can use the cross entropy loss, which often deliver better results in classification problems than the mean squared error.\n",
    "\n",
    "Finally you will need to define an optimizer that takes in the loss and update the weights of the neural network in the direction that minimizes the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating Logits\n",
    "h = train_inputs\n",
    "for lid in layer_ids:\n",
    "    with tf.variable_scope(lid,reuse=True):\n",
    "        w, b = tf.get_variable('weights'), tf.get_variable('bias')\n",
    "        if lid != 'out':\n",
    "          h = tf.nn.relu(tf.matmul(h,w)+b,name=lid+'_output')\n",
    "        else:\n",
    "          h = tf.nn.xw_plus_b(h,w,b,name=lid+'_output')\n",
    "\n",
    "tf_predictions = tf.nn.softmax(h, name='predictions')\n",
    "# Calculating Loss\n",
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=train_labels, logits=h),name='loss')\n",
    "\n",
    "# Optimizer \n",
    "tf_learning_rate = tf.placeholder(tf.float32, shape=None, name='learning_rate')\n",
    "optimizer = tf.train.MomentumOptimizer(tf_learning_rate,momentum=0.9)\n",
    "grads_and_vars = optimizer.compute_gradients(tf_loss)\n",
    "tf_loss_minimize = optimizer.minimize(tf_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tensorboard Summaries\n",
    "\n",
    "Here you can define the `tf.summary` objects. `tf.summary` objects are the type of entities understood by the Tensorboard. This means that whatever value you'd like to be displayed on the Tensorboard, you should encapsulate it as a `tf.summary` object. There are several different types of summaries. Here as you are visualizing only scalars, you can define `tf.summary.scalar` objects. Furthermore, you can use `tf.name_scope` to group scalars on the Tensorboard. That is, scalars having the same name scope will be displayed on the same row on the Tensorboard. Here you define three different summaries.\n",
    "\n",
    "* `tf_loss_summary` : You feed in a value by means of a placeholder, whenever you need to publish this to the Tensorboard\n",
    "* `tf_accuracy_summary` : You feed in a value by means of a placeholder, whenever you need to publish this to the Tensorboard\n",
    "* `tf_gradnorm_summary` : This calculates the l2 norm of the gradients of the last layer of your neural network. Gradient norm is a good indicator of whether the weights of the neural network are being properly updated. A too small gradient norm can indicate *vanishing gradient* or a too large gradient can imply *exploding gradient* phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name scope allows you to group various summaries together\n",
    "# Summaries having the same name_scope will be displayed on the same row on the Tensorboard\n",
    "with tf.name_scope('performance'):\n",
    "    # Summaries need to display on the Tensorboard\n",
    "    # Whenever need to record the loss, feed the mean loss to this placeholder\n",
    "    tf_loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary') \n",
    "    # Create a scalar summary object for the loss so Tensorboard knows how to display it\n",
    "    tf_loss_summary = tf.summary.scalar('loss', tf_loss_ph)\n",
    "\n",
    "    # Whenever you need to record the loss, feed the mean test accuracy to this placeholder\n",
    "    tf_accuracy_ph = tf.placeholder(tf.float32,shape=None, name='accuracy_summary') \n",
    "    # Create a scalar summary object for the accuracy so Tensorboard knows how to display it\n",
    "    tf_accuracy_summary = tf.summary.scalar('accuracy', tf_accuracy_ph)\n",
    "\n",
    "# Gradient norm summary\n",
    "for g,v in grads_and_vars:\n",
    "    if 'hidden5' in v.name and 'weights' in v.name:\n",
    "        with tf.name_scope('gradients'):\n",
    "            tf_last_grad_norm = tf.sqrt(tf.reduce_mean(g**2))\n",
    "            tf_gradnorm_summary = tf.summary.scalar('grad_norm', tf_last_grad_norm)\n",
    "            break\n",
    "# Merge all summaries together\n",
    "performance_summaries = tf.summary.merge([tf_loss_summary,tf_accuracy_summary])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the neural network model: Loading Data, Training, Validation and Testing\n",
    "\n",
    "In the code below you do the following. First you create a session, in which you execute the operations you defined above. Then you create folder for saving summary data. You next create a summary write `summ_writer`. You can now initialize all variables. This will be followed by loading the MNIST dataset.\n",
    "\n",
    "Then for each epoch, and each batch in training data (that is, each iteration). Execute `gradnorm_summary` if it is the first iteration and write `gradnorm_summary` to event file with summary writer. You now execute model optimization and calculating the loss. After you go through the full training dataset for a single epoch, calculate average training loss.\n",
    "\n",
    "You follow a similar treatment for the validation dataset as well. Specifically, for each batch in validation data, you calculate validation accuracy for each batch. Thereafter calculate average validation accuracy for full validation set.\n",
    "\n",
    "Finally, the testing phase is executed. In this, for each batch in test data, you calculate test accuracy for each batch. With that, you calculate average test accuracy for full test set. At the very end you execute `performance_summaries` and write them to event file with the summary writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss in epoch 0: 2.30480\n",
      "\tAverage Valid Accuracy in epoch 0: 8.24000\n",
      "\tAverage Test Accuracy in epoch 0: 8.67000\n",
      "\n",
      "Average loss in epoch 1: 2.30277\n",
      "\tAverage Valid Accuracy in epoch 1: 8.52000\n",
      "\tAverage Test Accuracy in epoch 1: 8.87000\n",
      "\n",
      "Average loss in epoch 2: 2.30079\n",
      "\tAverage Valid Accuracy in epoch 2: 8.84000\n",
      "\tAverage Test Accuracy in epoch 2: 9.21000\n",
      "\n",
      "Average loss in epoch 3: 2.29883\n",
      "\tAverage Valid Accuracy in epoch 3: 9.36000\n",
      "\tAverage Test Accuracy in epoch 3: 9.64000\n",
      "\n",
      "Average loss in epoch 4: 2.29682\n",
      "\tAverage Valid Accuracy in epoch 4: 10.68000\n",
      "\tAverage Test Accuracy in epoch 4: 10.63000\n",
      "\n",
      "Average loss in epoch 5: 2.29473\n",
      "\tAverage Valid Accuracy in epoch 5: 17.64000\n",
      "\tAverage Test Accuracy in epoch 5: 16.85000\n",
      "\n",
      "Average loss in epoch 6: 2.29249\n",
      "\tAverage Valid Accuracy in epoch 6: 24.02000\n",
      "\tAverage Test Accuracy in epoch 6: 23.65000\n",
      "\n",
      "Average loss in epoch 7: 2.29005\n",
      "\tAverage Valid Accuracy in epoch 7: 26.64000\n",
      "\tAverage Test Accuracy in epoch 7: 25.64000\n",
      "\n",
      "Average loss in epoch 8: 2.28732\n",
      "\tAverage Valid Accuracy in epoch 8: 27.64000\n",
      "\tAverage Test Accuracy in epoch 8: 26.53000\n",
      "\n",
      "Average loss in epoch 9: 2.28420\n",
      "\tAverage Valid Accuracy in epoch 9: 28.06000\n",
      "\tAverage Test Accuracy in epoch 9: 27.22000\n",
      "\n",
      "Average loss in epoch 10: 2.28056\n",
      "\tAverage Valid Accuracy in epoch 10: 28.84000\n",
      "\tAverage Test Accuracy in epoch 10: 28.27000\n",
      "\n",
      "Average loss in epoch 11: 2.27626\n",
      "\tAverage Valid Accuracy in epoch 11: 29.80000\n",
      "\tAverage Test Accuracy in epoch 11: 29.24000\n",
      "\n",
      "Average loss in epoch 12: 2.27104\n",
      "\tAverage Valid Accuracy in epoch 12: 31.48000\n",
      "\tAverage Test Accuracy in epoch 12: 31.13000\n",
      "\n",
      "Average loss in epoch 13: 2.26458\n",
      "\tAverage Valid Accuracy in epoch 13: 34.22000\n",
      "\tAverage Test Accuracy in epoch 13: 33.76000\n",
      "\n",
      "Average loss in epoch 14: 2.25642\n",
      "\tAverage Valid Accuracy in epoch 14: 38.02000\n",
      "\tAverage Test Accuracy in epoch 14: 37.85000\n",
      "\n",
      "Average loss in epoch 15: 2.24581\n",
      "\tAverage Valid Accuracy in epoch 15: 41.84000\n",
      "\tAverage Test Accuracy in epoch 15: 41.75000\n",
      "\n",
      "Average loss in epoch 16: 2.23146\n",
      "\tAverage Valid Accuracy in epoch 16: 45.04000\n",
      "\tAverage Test Accuracy in epoch 16: 44.63000\n",
      "\n",
      "Average loss in epoch 17: 2.21123\n",
      "\tAverage Valid Accuracy in epoch 17: 47.72000\n",
      "\tAverage Test Accuracy in epoch 17: 46.60000\n",
      "\n",
      "Average loss in epoch 18: 2.18142\n",
      "\tAverage Valid Accuracy in epoch 18: 48.84000\n",
      "\tAverage Test Accuracy in epoch 18: 47.44000\n",
      "\n",
      "Average loss in epoch 19: 2.13528\n",
      "\tAverage Valid Accuracy in epoch 19: 48.54000\n",
      "\tAverage Test Accuracy in epoch 19: 47.68000\n",
      "\n",
      "Average loss in epoch 20: 2.06068\n",
      "\tAverage Valid Accuracy in epoch 20: 45.86000\n",
      "\tAverage Test Accuracy in epoch 20: 45.84000\n",
      "\n",
      "Average loss in epoch 21: 1.94296\n",
      "\tAverage Valid Accuracy in epoch 21: 43.86000\n",
      "\tAverage Test Accuracy in epoch 21: 43.45000\n",
      "\n",
      "Average loss in epoch 22: 1.78510\n",
      "\tAverage Valid Accuracy in epoch 22: 47.84000\n",
      "\tAverage Test Accuracy in epoch 22: 47.24000\n",
      "\n",
      "Average loss in epoch 23: 1.60649\n",
      "\tAverage Valid Accuracy in epoch 23: 52.72000\n",
      "\tAverage Test Accuracy in epoch 23: 52.87000\n",
      "\n",
      "Average loss in epoch 24: 1.42325\n",
      "\tAverage Valid Accuracy in epoch 24: 59.08000\n",
      "\tAverage Test Accuracy in epoch 24: 58.69000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_size = 28\n",
    "n_channels = 1\n",
    "n_classes = 10\n",
    "n_train = 55000\n",
    "n_valid = 5000\n",
    "n_test = 10000\n",
    "n_epochs = 25\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9 # making sure Tensorflow doesn't overflow the GPU\n",
    "\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "if not os.path.exists('summaries'):\n",
    "    os.mkdir('summaries')\n",
    "if not os.path.exists(os.path.join('summaries','first')):\n",
    "    os.mkdir(os.path.join('summaries','first'))\n",
    "\n",
    "summ_writer = tf.summary.FileWriter(os.path.join('summaries','first'), session.graph)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "accuracy_per_epoch = []\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_per_epoch = []\n",
    "    for i in range(n_train//batch_size):\n",
    "        \n",
    "        # =================================== Training for one step ========================================\n",
    "        batch = mnist_data.train.next_batch(batch_size)    # Get one batch of training data\n",
    "        if i == 0:\n",
    "            # Only for the first epoch, get the summary data\n",
    "            # Otherwise, it can clutter the visualization\n",
    "            l,_,gn_summ = session.run([tf_loss,tf_loss_minimize,tf_gradnorm_summary],\n",
    "                                      feed_dict={train_inputs: batch[0].reshape(batch_size,image_size*image_size),\n",
    "                                                 train_labels: batch[1],\n",
    "                                                tf_learning_rate: 0.0001})\n",
    "            summ_writer.add_summary(gn_summ, epoch)\n",
    "        else:\n",
    "            # Optimize with training data\n",
    "            l,_ = session.run([tf_loss,tf_loss_minimize],\n",
    "                              feed_dict={train_inputs: batch[0].reshape(batch_size,image_size*image_size),\n",
    "                                         train_labels: batch[1],\n",
    "                                         tf_learning_rate: 0.0001})\n",
    "        loss_per_epoch.append(l)\n",
    "        \n",
    "    print('Average loss in epoch %d: %.5f'%(epoch,np.mean(loss_per_epoch)))    \n",
    "    avg_loss = np.mean(loss_per_epoch)\n",
    "    \n",
    "    # ====================== Calculate the Validation Accuracy ==========================\n",
    "    valid_accuracy_per_epoch = []\n",
    "    for i in range(n_valid//batch_size):\n",
    "        valid_images,valid_labels = mnist_data.validation.next_batch(batch_size)\n",
    "        valid_batch_predictions = session.run(\n",
    "            tf_predictions,feed_dict={train_inputs: valid_images.reshape(batch_size,image_size*image_size)})\n",
    "        valid_accuracy_per_epoch.append(accuracy(valid_batch_predictions,valid_labels))\n",
    "        \n",
    "    mean_v_acc = np.mean(valid_accuracy_per_epoch)\n",
    "    print('\\tAverage Valid Accuracy in epoch %d: %.5f'%(epoch,np.mean(valid_accuracy_per_epoch)))\n",
    "    \n",
    "    # ===================== Calculate the Test Accuracy ===============================\n",
    "    accuracy_per_epoch = []\n",
    "    for i in range(n_test//batch_size):\n",
    "        test_images, test_labels = mnist_data.test.next_batch(batch_size)\n",
    "        test_batch_predictions = session.run(\n",
    "            tf_predictions,feed_dict={train_inputs: test_images.reshape(batch_size,image_size*image_size)}\n",
    "        )\n",
    "        accuracy_per_epoch.append(accuracy(test_batch_predictions,test_labels))\n",
    "        \n",
    "    print('\\tAverage Test Accuracy in epoch %d: %.5f\\n'%(epoch,np.mean(accuracy_per_epoch)))\n",
    "    avg_test_accuracy = np.mean(accuracy_per_epoch)\n",
    "    \n",
    "    # Execute the summaries defined above\n",
    "    summ = session.run(performance_summaries, feed_dict={tf_loss_ph:avg_loss, tf_accuracy_ph:avg_test_accuracy})\n",
    "\n",
    "    # Write the obtained summaries to the file, so it can be displayed in the Tensorboard\n",
    "    summ_writer.add_summary(summ, epoch)\n",
    "    \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualizing the Computational Graph\n",
    "First you will see what the computational graph for our model looks like. You can access this view by clicking on the **Graphs** view on the Tensorboard. It should look like something below. You can see that you have a nice flow from `train_inputs` to `loss` and `predictions` flowing through the **hidden layers** **1** to **5**.\n",
    "\n",
    "<img src=\"tensorboard_graph.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Visualizing the Summary Data: Is Everything A-Okey Here?\n",
    "\n",
    "MNIST classification is one of the simplest examples, and still cannot solve it with a 5 layer neural network. For MNIST, it's not difficult to achieve an accuracy of more than 90% in less than 5 epochs. So what is  going on here? Let's turn towards our Tensorboard.\n",
    "\n",
    "This is what the Tensorboard looks like for our example.\n",
    "<img src=\"tensorboard_1.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Observations and Conclusions from the Tensorboard\n",
    "You can see that the accuracy is going up, but very slow. You can see that the gradients updates are increasing over time. This is an odd behavior. If you're reaching towards convergence, you should see the gradients diminishing (approaching zero), not increasing.  But because the accuracy is going up, we're on the right path. *You probably need a higher learning rate*. You can now try a learning rate of **0.01**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using a Higher Learning Rate and Executing the Neural Network Model\n",
    "\n",
    "In the code below you do the following. First you create a session, in which you execute the operations you defined above. Then you create folder for saving summary data. You next create a summary write `summ_writer`. You can now initialize all variables. This will be followed by loading the MNIST dataset.\n",
    "\n",
    "Then for each epoch, and each batch in training data (that is, each iteration). Execute `gradnorm_summary` if it is the first iteration and write `gradnorm_summary` to event file with summary writer. You now execute model optimization and calculating the loss. After you go through the full training dataset for a single epoch, calculate average training loss.\n",
    "\n",
    "You follow a similar treatment for the validation dataset as well. Specifically, for each batch in validation data, you calculate validation accuracy for each batch. Thereafter calculate average validation accuracy for full validation set.\n",
    "\n",
    "Finally, the testing phase is executed. In this, for each batch in test data, you calculate test accuracy for each batch. With that, you calculate average test accuracy for full test set. At the very end you execute `performance_summaries` and write them to event file with the summary writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss in epoch 0: 0.96963\n",
      "\tAverage Valid Accuracy in epoch 0: 93.56000\n",
      "\tAverage Test Accuracy in epoch 0: 92.70000\n",
      "\n",
      "Average loss in epoch 1: 0.18322\n",
      "\tAverage Valid Accuracy in epoch 1: 94.68000\n",
      "\tAverage Test Accuracy in epoch 1: 94.10000\n",
      "\n",
      "Average loss in epoch 2: 0.11146\n",
      "\tAverage Valid Accuracy in epoch 2: 97.10000\n",
      "\tAverage Test Accuracy in epoch 2: 96.59000\n",
      "\n",
      "Average loss in epoch 3: 0.07916\n",
      "\tAverage Valid Accuracy in epoch 3: 97.02000\n",
      "\tAverage Test Accuracy in epoch 3: 96.59000\n",
      "\n",
      "Average loss in epoch 4: 0.05842\n",
      "\tAverage Valid Accuracy in epoch 4: 97.74000\n",
      "\tAverage Test Accuracy in epoch 4: 97.38000\n",
      "\n",
      "Average loss in epoch 5: 0.04314\n",
      "\tAverage Valid Accuracy in epoch 5: 97.72000\n",
      "\tAverage Test Accuracy in epoch 5: 97.45000\n",
      "\n",
      "Average loss in epoch 6: 0.03279\n",
      "\tAverage Valid Accuracy in epoch 6: 98.08000\n",
      "\tAverage Test Accuracy in epoch 6: 97.83000\n",
      "\n",
      "Average loss in epoch 7: 0.02241\n",
      "\tAverage Valid Accuracy in epoch 7: 97.94000\n",
      "\tAverage Test Accuracy in epoch 7: 97.72000\n",
      "\n",
      "Average loss in epoch 8: 0.01907\n",
      "\tAverage Valid Accuracy in epoch 8: 97.98000\n",
      "\tAverage Test Accuracy in epoch 8: 97.72000\n",
      "\n",
      "Average loss in epoch 9: 0.01381\n",
      "\tAverage Valid Accuracy in epoch 9: 98.08000\n",
      "\tAverage Test Accuracy in epoch 9: 97.81000\n",
      "\n",
      "Average loss in epoch 10: 0.01153\n",
      "\tAverage Valid Accuracy in epoch 10: 97.80000\n",
      "\tAverage Test Accuracy in epoch 10: 97.40000\n",
      "\n",
      "Average loss in epoch 11: 0.00779\n",
      "\tAverage Valid Accuracy in epoch 11: 98.20000\n",
      "\tAverage Test Accuracy in epoch 11: 97.76000\n",
      "\n",
      "Average loss in epoch 12: 0.00602\n",
      "\tAverage Valid Accuracy in epoch 12: 97.92000\n",
      "\tAverage Test Accuracy in epoch 12: 97.79000\n",
      "\n",
      "Average loss in epoch 13: 0.00622\n",
      "\tAverage Valid Accuracy in epoch 13: 98.08000\n",
      "\tAverage Test Accuracy in epoch 13: 97.97000\n",
      "\n",
      "Average loss in epoch 14: 0.00187\n",
      "\tAverage Valid Accuracy in epoch 14: 98.18000\n",
      "\tAverage Test Accuracy in epoch 14: 97.99000\n",
      "\n",
      "Average loss in epoch 15: 0.00191\n",
      "\tAverage Valid Accuracy in epoch 15: 98.24000\n",
      "\tAverage Test Accuracy in epoch 15: 97.99000\n",
      "\n",
      "Average loss in epoch 16: 0.00057\n",
      "\tAverage Valid Accuracy in epoch 16: 98.36000\n",
      "\tAverage Test Accuracy in epoch 16: 98.22000\n",
      "\n",
      "Average loss in epoch 17: 0.00037\n",
      "\tAverage Valid Accuracy in epoch 17: 98.34000\n",
      "\tAverage Test Accuracy in epoch 17: 98.14000\n",
      "\n",
      "Average loss in epoch 18: 0.00017\n",
      "\tAverage Valid Accuracy in epoch 18: 98.34000\n",
      "\tAverage Test Accuracy in epoch 18: 98.14000\n",
      "\n",
      "Average loss in epoch 19: 0.00014\n",
      "\tAverage Valid Accuracy in epoch 19: 98.36000\n",
      "\tAverage Test Accuracy in epoch 19: 98.20000\n",
      "\n",
      "Average loss in epoch 20: 0.00012\n",
      "\tAverage Valid Accuracy in epoch 20: 98.38000\n",
      "\tAverage Test Accuracy in epoch 20: 98.16000\n",
      "\n",
      "Average loss in epoch 21: 0.00010\n",
      "\tAverage Valid Accuracy in epoch 21: 98.40000\n",
      "\tAverage Test Accuracy in epoch 21: 98.18000\n",
      "\n",
      "Average loss in epoch 22: 0.00009\n",
      "\tAverage Valid Accuracy in epoch 22: 98.36000\n",
      "\tAverage Test Accuracy in epoch 22: 98.18000\n",
      "\n",
      "Average loss in epoch 23: 0.00009\n",
      "\tAverage Valid Accuracy in epoch 23: 98.36000\n",
      "\tAverage Test Accuracy in epoch 23: 98.17000\n",
      "\n",
      "Average loss in epoch 24: 0.00008\n",
      "\tAverage Valid Accuracy in epoch 24: 98.38000\n",
      "\tAverage Test Accuracy in epoch 24: 98.17000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_size = 28\n",
    "n_channels = 1\n",
    "n_classes = 10\n",
    "n_train = 55000\n",
    "n_valid = 5000\n",
    "n_test = 10000\n",
    "n_epochs = 25\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9 # making sure Tensorflow doesn't overflow the GPU\n",
    "\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "if not os.path.exists('summaries'):\n",
    "    os.mkdir('summaries')\n",
    "if not os.path.exists(os.path.join('summaries','second')):\n",
    "    os.mkdir(os.path.join('summaries','second'))\n",
    "    \n",
    "summ_writer_2 = tf.summary.FileWriter(os.path.join('summaries','second'), session.graph)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "accuracy_per_epoch = []\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_per_epoch = []\n",
    "    for i in range(n_train//batch_size):\n",
    "        \n",
    "        # =================================== Training for one step ========================================\n",
    "        batch = mnist_data.train.next_batch(batch_size)    # Get one batch of training data\n",
    "        if i == 0:\n",
    "            # Only for the first epoch, get the summary data\n",
    "            # Otherwise, it can clutter the visualization\n",
    "            l,_,gn_summ = session.run([tf_loss,tf_loss_minimize,tf_gradnorm_summary],\n",
    "                                      feed_dict={train_inputs: batch[0].reshape(batch_size,image_size*image_size),\n",
    "                                                 train_labels: batch[1],\n",
    "                                                tf_learning_rate: 0.01})\n",
    "            summ_writer_2.add_summary(gn_summ, epoch)\n",
    "        else:\n",
    "            # Optimize with training data\n",
    "            l,_ = session.run([tf_loss,tf_loss_minimize],\n",
    "                              feed_dict={train_inputs: batch[0].reshape(batch_size,image_size*image_size),\n",
    "                                         train_labels: batch[1],\n",
    "                                         tf_learning_rate: 0.01})\n",
    "        loss_per_epoch.append(l)\n",
    "        \n",
    "    print('Average loss in epoch %d: %.5f'%(epoch,np.mean(loss_per_epoch)))    \n",
    "    avg_loss = np.mean(loss_per_epoch)\n",
    "    \n",
    "    # ====================== Calculate the Validation Accuracy ==========================\n",
    "    valid_accuracy_per_epoch = []\n",
    "    for i in range(n_valid//batch_size):\n",
    "        valid_images,valid_labels = mnist_data.validation.next_batch(batch_size)\n",
    "        valid_batch_predictions = session.run(\n",
    "            tf_predictions,feed_dict={train_inputs: valid_images.reshape(batch_size,image_size*image_size)})\n",
    "        valid_accuracy_per_epoch.append(accuracy(valid_batch_predictions,valid_labels))\n",
    "        \n",
    "    mean_v_acc = np.mean(valid_accuracy_per_epoch)\n",
    "    print('\\tAverage Valid Accuracy in epoch %d: %.5f'%(epoch,np.mean(valid_accuracy_per_epoch)))\n",
    "    \n",
    "    # ===================== Calculate the Test Accuracy ===============================\n",
    "    accuracy_per_epoch = []\n",
    "    for i in range(n_test//batch_size):\n",
    "        test_images, test_labels = mnist_data.test.next_batch(batch_size)\n",
    "        test_batch_predictions = session.run(\n",
    "            tf_predictions,feed_dict={train_inputs: test_images.reshape(batch_size,image_size*image_size)}\n",
    "        )\n",
    "        accuracy_per_epoch.append(accuracy(test_batch_predictions,test_labels))\n",
    "        \n",
    "    print('\\tAverage Test Accuracy in epoch %d: %.5f\\n'%(epoch,np.mean(accuracy_per_epoch)))\n",
    "    avg_test_accuracy = np.mean(accuracy_per_epoch)\n",
    "    \n",
    "    # Execute the summaries defined above\n",
    "    summ = session.run(performance_summaries, feed_dict={tf_loss_ph:avg_loss, tf_accuracy_ph:avg_test_accuracy})\n",
    "\n",
    "    # Write the obtained summaries to the file, so it can be displayed in the Tensorboard\n",
    "    summ_writer_2.add_summary(summ, epoch)\n",
    "    \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Look at the Tensorboard: Looks Much Better Now\n",
    "\n",
    "Now you can see that the accuracy starts close to 100 and continues to go up. And you can see that the gradient updates are also diminishing over time and approaches zero. Things seems much better with the learning rate of 0.01.\n",
    "\n",
    "<img src=\"tensorboard_2.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Next let's move beyond scalars. You will see how you can analyze vectors/collections of scalars with the Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Scalars: Visualizing Histograms/Distributions through Tensorboard\n",
    "\n",
    "You saw the benefit of visualizing scalars through Tensorboard, which allowed us to see how the model behaves and fix any potential issues with the model. Moreover, visualizing the graph allowed us to see that there is an uninterrupted link from the inputs to the predictions, which is necessary for gradient calcualtions. \n",
    "\n",
    "Now we're going to see another useful view in Tensorboard; histograms or distributions. Histograms means what it means! It is a collection of values represented by the frequency/density that the value is present in the collection. How can you can use histograms to visualize something in the neural network. You can use histograms to visualize the network weight values over time. Visualizing network weights is important, because if the weights are wildly jumping here and their during learning, it indicates something wrong with the weight initialization or the learning rate. You will see how weights change in our example. If you look at the code, it uses a *truncated_normal_initializer(...)* to initialize weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Defining Histogram Summaries to Visualize Weights and Biases\n",
    "\n",
    "Here you again define the `tf.summary` objects. However now you are visualizing vectors of scalars so you need to define `tf.summary.histogram` objects. In this case, you define two histogram objects (namely, `tf_w_hist` and `tf_b_hist`) that contains weights and biases of agiven layer. You will define such histogram objects for all the layers and each layer will have it's own name scope. Finally you can use the `tf.summary.merge` operation to create a grouped operation that execute all these summaries at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Summaries need to display on the Tensorboard\n",
    "# Create a summary for each weight bias in each layer\n",
    "all_summaries = []\n",
    "for lid in layer_ids:\n",
    "    with tf.name_scope(lid+'_hist'):\n",
    "        with tf.variable_scope(lid,reuse=True):\n",
    "            w,b = tf.get_variable('weights'), tf.get_variable('bias')\n",
    "\n",
    "            # Create a scalar summary object for the loss so Tensorboard knows how to display it\n",
    "            tf_w_hist = tf.summary.histogram('weights_hist', tf.reshape(w,[-1]))\n",
    "            tf_b_hist = tf.summary.histogram('bias_hist', b)\n",
    "            all_summaries.extend([tf_w_hist, tf_b_hist])\n",
    "\n",
    "# Merge all parameter histogram summaries together\n",
    "tf_param_summaries = tf.summary.merge(all_summaries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the neural network model (with Histogram Summaries)\n",
    "\n",
    "In the code below you do the following. First you create a session, in which you execute the operations you defined above. Then you create folder for saving summary data. You next create a summary write `summ_writer`. You can now initialize all variables. This will be followed by loading the MNIST dataset.\n",
    "\n",
    "Then for each epoch, and each batch in training data (that is, each iteration). Execute `gradnorm_summary` and `tf_param_summaries` if it is the first iteration and write `gradnorm_summary` and `tf_param_summaries` to event file with summary writer. You now execute model optimization and calculating the loss. After you go through the full training dataset for a single epoch, calculate average training loss.\n",
    "\n",
    "You follow a similar treatment for the validation dataset as well. Specifically, for each batch in validation data, you calculate validation accuracy for each batch. Thereafter calculate average validation accuracy for full validation set.\n",
    "\n",
    "Finally, the testing phase is executed. In this, for each batch in test data, you calculate test accuracy for each batch. With that, you calculate average test accuracy for full test set. At the very end you execute `performance_summaries` and write them to event file with the summary writer.\n",
    "\n",
    "**Note**: This is as same as you did before, but here you have few additional line to compute the histogram summaries (that is, `tf_param_summaries`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss in epoch 0: 0.94838\n",
      "\tAverage Valid Accuracy in epoch 0: 93.76000\n",
      "\tAverage Test Accuracy in epoch 0: 93.41000\n",
      "\n",
      "Average loss in epoch 1: 0.17797\n",
      "\tAverage Valid Accuracy in epoch 1: 96.00000\n",
      "\tAverage Test Accuracy in epoch 1: 95.43000\n",
      "\n",
      "Average loss in epoch 2: 0.11237\n",
      "\tAverage Valid Accuracy in epoch 2: 96.84000\n",
      "\tAverage Test Accuracy in epoch 2: 96.67000\n",
      "\n",
      "Average loss in epoch 3: 0.07718\n",
      "\tAverage Valid Accuracy in epoch 3: 97.36000\n",
      "\tAverage Test Accuracy in epoch 3: 97.08000\n",
      "\n",
      "Average loss in epoch 4: 0.05755\n",
      "\tAverage Valid Accuracy in epoch 4: 97.64000\n",
      "\tAverage Test Accuracy in epoch 4: 97.63000\n",
      "\n",
      "Average loss in epoch 5: 0.04365\n",
      "\tAverage Valid Accuracy in epoch 5: 97.78000\n",
      "\tAverage Test Accuracy in epoch 5: 97.41000\n",
      "\n",
      "Average loss in epoch 6: 0.03195\n",
      "\tAverage Valid Accuracy in epoch 6: 97.60000\n",
      "\tAverage Test Accuracy in epoch 6: 97.42000\n",
      "\n",
      "Average loss in epoch 7: 0.02522\n",
      "\tAverage Valid Accuracy in epoch 7: 97.88000\n",
      "\tAverage Test Accuracy in epoch 7: 97.74000\n",
      "\n",
      "Average loss in epoch 8: 0.01883\n",
      "\tAverage Valid Accuracy in epoch 8: 97.94000\n",
      "\tAverage Test Accuracy in epoch 8: 97.71000\n",
      "\n",
      "Average loss in epoch 9: 0.01504\n",
      "\tAverage Valid Accuracy in epoch 9: 97.70000\n",
      "\tAverage Test Accuracy in epoch 9: 97.39000\n",
      "\n",
      "Average loss in epoch 10: 0.01283\n",
      "\tAverage Valid Accuracy in epoch 10: 98.00000\n",
      "\tAverage Test Accuracy in epoch 10: 97.77000\n",
      "\n",
      "Average loss in epoch 11: 0.00796\n",
      "\tAverage Valid Accuracy in epoch 11: 98.24000\n",
      "\tAverage Test Accuracy in epoch 11: 97.91000\n",
      "\n",
      "Average loss in epoch 12: 0.00872\n",
      "\tAverage Valid Accuracy in epoch 12: 97.98000\n",
      "\tAverage Test Accuracy in epoch 12: 97.79000\n",
      "\n",
      "Average loss in epoch 13: 0.00390\n",
      "\tAverage Valid Accuracy in epoch 13: 98.18000\n",
      "\tAverage Test Accuracy in epoch 13: 98.03000\n",
      "\n",
      "Average loss in epoch 14: 0.00113\n",
      "\tAverage Valid Accuracy in epoch 14: 98.32000\n",
      "\tAverage Test Accuracy in epoch 14: 98.16000\n",
      "\n",
      "Average loss in epoch 15: 0.00071\n",
      "\tAverage Valid Accuracy in epoch 15: 98.16000\n",
      "\tAverage Test Accuracy in epoch 15: 98.07000\n",
      "\n",
      "Average loss in epoch 16: 0.00057\n",
      "\tAverage Valid Accuracy in epoch 16: 98.26000\n",
      "\tAverage Test Accuracy in epoch 16: 98.17000\n",
      "\n",
      "Average loss in epoch 17: 0.00039\n",
      "\tAverage Valid Accuracy in epoch 17: 98.10000\n",
      "\tAverage Test Accuracy in epoch 17: 98.19000\n",
      "\n",
      "Average loss in epoch 18: 0.00018\n",
      "\tAverage Valid Accuracy in epoch 18: 98.18000\n",
      "\tAverage Test Accuracy in epoch 18: 98.13000\n",
      "\n",
      "Average loss in epoch 19: 0.00015\n",
      "\tAverage Valid Accuracy in epoch 19: 98.14000\n",
      "\tAverage Test Accuracy in epoch 19: 98.14000\n",
      "\n",
      "Average loss in epoch 20: 0.00013\n",
      "\tAverage Valid Accuracy in epoch 20: 98.20000\n",
      "\tAverage Test Accuracy in epoch 20: 98.10000\n",
      "\n",
      "Average loss in epoch 21: 0.00011\n",
      "\tAverage Valid Accuracy in epoch 21: 98.16000\n",
      "\tAverage Test Accuracy in epoch 21: 98.13000\n",
      "\n",
      "Average loss in epoch 22: 0.00010\n",
      "\tAverage Valid Accuracy in epoch 22: 98.14000\n",
      "\tAverage Test Accuracy in epoch 22: 98.12000\n",
      "\n",
      "Average loss in epoch 23: 0.00009\n",
      "\tAverage Valid Accuracy in epoch 23: 98.16000\n",
      "\tAverage Test Accuracy in epoch 23: 98.15000\n",
      "\n",
      "Average loss in epoch 24: 0.00009\n",
      "\tAverage Valid Accuracy in epoch 24: 98.18000\n",
      "\tAverage Test Accuracy in epoch 24: 98.14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_size = 28\n",
    "n_channels = 1\n",
    "n_classes = 10\n",
    "n_train = 55000\n",
    "n_valid = 5000\n",
    "n_test = 10000\n",
    "n_epochs = 25\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9 # making sure Tensorflow doesn't overflow the GPU\n",
    "\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "if not os.path.exists('summaries'):\n",
    "    os.mkdir('summaries')\n",
    "if not os.path.exists(os.path.join('summaries','third')):\n",
    "    os.mkdir(os.path.join('summaries','third'))\n",
    "    \n",
    "summ_writer_3 = tf.summary.FileWriter(os.path.join('summaries','third'), session.graph)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "accuracy_per_epoch = []\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_per_epoch = []\n",
    "    for i in range(n_train//batch_size):\n",
    "        \n",
    "        # =================================== Training for one step ========================================\n",
    "        batch = mnist_data.train.next_batch(batch_size)    # Get one batch of training data\n",
    "        if i == 0:\n",
    "            # Only for the first epoch, get the summary data\n",
    "            # Otherwise, it can clutter the visualization\n",
    "            l,_,gn_summ, wb_summ = session.run([tf_loss,tf_loss_minimize,tf_gradnorm_summary, tf_param_summaries],\n",
    "                                      feed_dict={train_inputs: batch[0].reshape(batch_size,image_size*image_size),\n",
    "                                                 train_labels: batch[1],\n",
    "                                                tf_learning_rate: 0.00001})\n",
    "            summ_writer_3.add_summary(gn_summ, epoch)\n",
    "            summ_writer_3.add_summary(wb_summ, epoch)\n",
    "        else:\n",
    "            # Optimize with training data\n",
    "            l,_ = session.run([tf_loss,tf_loss_minimize],\n",
    "                              feed_dict={train_inputs: batch[0].reshape(batch_size,image_size*image_size),\n",
    "                                         train_labels: batch[1],\n",
    "                                         tf_learning_rate: 0.01})\n",
    "        loss_per_epoch.append(l)\n",
    "        \n",
    "    print('Average loss in epoch %d: %.5f'%(epoch,np.mean(loss_per_epoch)))    \n",
    "    avg_loss = np.mean(loss_per_epoch)\n",
    "    \n",
    "    # ====================== Calculate the Validation Accuracy ==========================\n",
    "    valid_accuracy_per_epoch = []\n",
    "    for i in range(n_valid//batch_size):\n",
    "        valid_images,valid_labels = mnist_data.validation.next_batch(batch_size)\n",
    "        valid_batch_predictions = session.run(\n",
    "            tf_predictions,feed_dict={train_inputs: valid_images.reshape(batch_size,image_size*image_size)})\n",
    "        valid_accuracy_per_epoch.append(accuracy(valid_batch_predictions,valid_labels))\n",
    "        \n",
    "    mean_v_acc = np.mean(valid_accuracy_per_epoch)\n",
    "    print('\\tAverage Valid Accuracy in epoch %d: %.5f'%(epoch,np.mean(valid_accuracy_per_epoch)))\n",
    "    \n",
    "    # ===================== Calculate the Test Accuracy ===============================\n",
    "    accuracy_per_epoch = []\n",
    "    for i in range(n_test//batch_size):\n",
    "        test_images, test_labels = mnist_data.test.next_batch(batch_size)\n",
    "        test_batch_predictions = session.run(\n",
    "            tf_predictions,feed_dict={train_inputs: test_images.reshape(batch_size,image_size*image_size)}\n",
    "        )\n",
    "        accuracy_per_epoch.append(accuracy(test_batch_predictions,test_labels))\n",
    "        \n",
    "    print('\\tAverage Test Accuracy in epoch %d: %.5f\\n'%(epoch,np.mean(accuracy_per_epoch)))\n",
    "    avg_test_accuracy = np.mean(accuracy_per_epoch)\n",
    "    \n",
    "    # Execute the summaries defined above\n",
    "    summ = session.run(performance_summaries, feed_dict={tf_loss_ph:avg_loss, tf_accuracy_ph:avg_test_accuracy})\n",
    "\n",
    "    # Write the obtained summaries to the file, so it can be displayed in the Tensorboard\n",
    "    summ_writer_3.add_summary(summ, epoch)\n",
    "    \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Histogram Data of Weights and Biases\n",
    "\n",
    "Here's what our weights and biases look like. First you have 3 axis; time (x axis), value (y axis) and frequency/density of values (z axis). Darker histograms represent old data and lighter historgrams represent newer data. Higher value on the z axis means that the vector contains more values near that specific value.\n",
    "\n",
    "**Note**: You also have an \"overlay\" view of the histograms over time as well. You can change the type of display on the left side option panel. \n",
    "\n",
    "<img src=\"tensorboard_3_1.png\" style=\"width: 400px;\"/>\n",
    "<img src=\"tensorboard_3_2.png\" style=\"width: 400px;\"/>\n",
    "<img src=\"tensorboard_3_3.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Different Initializers: Changing the Initialization of Weights and Re-Defining the Model\n",
    "\n",
    "Now, instead of using `truncated_normal_initializer()`, you will use the `xavier_initialzer()` to initialize weights. Xavier initialization is much better initialization technique, especially for deep neural networks. This is because Xavier initializer, instead of using a user defined standard deviation (as you did when using the `truncated_normal_initializer()`), Xaiver initialization automatically decides the standard deviation based on the number of input and output connections to a layer. This helps to flow gradients from top to bottom without issues like *vanishing gradient*. You then define the model again.\n",
    "\n",
    "First you define a `batch_size` denoting the amount of data you sample at a single optimization/validation or testing step. Then you define the `layer_ids`, which gives an identifier for each of the layers of the neural network you will be defining. You then can define `layer_sizes`. Note that `len(layer_sizes)` should be `len(layer_ids)+1`, because `layer_sizes` includes the size of the input at the beginning. MNIST has images of size 28x28, which will be 784 when unwrapped to a single dimension. Then you can define the input and label placeholders, that you will later use to train the model. Finally you define two TensorFlow variables for each layer (that is, `weights` and `bias`).\n",
    "\n",
    "**Note**: This is identical to the code you used first time, except for the initialization technique used for the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "layer_ids = ['hidden1','hidden2','hidden3','hidden4','hidden5','out']\n",
    "layer_sizes = [784, 500, 400, 300, 200, 100, 10]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs and Labels\n",
    "train_inputs = tf.placeholder(tf.float32, shape=[batch_size, layer_sizes[0]], name='train_inputs')\n",
    "train_labels = tf.placeholder(tf.float32, shape=[batch_size, layer_sizes[-1]], name='train_labels')\n",
    "\n",
    "# Weight and Bias definitions\n",
    "for idx, lid in enumerate(layer_ids):\n",
    "    \n",
    "    with tf.variable_scope(lid):\n",
    "        w = tf.get_variable('weights',shape=[layer_sizes[idx], layer_sizes[idx+1]], \n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('bias',shape= [layer_sizes[idx+1]], \n",
    "                            initializer=tf.random_uniform_initializer(-0.1,0.1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Logits, Predictions, Loss and Optimization\n",
    "\n",
    "With the input/output placeholders, weights and biases of each layer defined, you now can define the calculations to calculate the logits of the neural network. Logits are the unnormalized values produced at the last layer of the neural network. When normalized, you call them predictions. This involves iterating through each layer in the neural network and computing `tf.matmul(h,w) +b`. You also need to apply an activation function as `tf.nn.relu(tf.matmul(h,w) +b)`, for all layers except for the last layer.\n",
    "\n",
    "Next you define loss function that is used to optimize the neural network. In this example, you can use the cross entropy loss, which often deliver better results in classification problems than the mean squared error.\n",
    "\n",
    "Finally you will need to define an optimizer that takes in the loss and update the weights of the neural network in the direction that minimizes the loss.\n",
    "\n",
    "**Note**: This is identical to the code you used first time you defined these operations and tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating Logits\n",
    "h = train_inputs\n",
    "for lid in layer_ids:\n",
    "    with tf.variable_scope(lid,reuse=True):\n",
    "        w, b = tf.get_variable('weights'), tf.get_variable('bias')\n",
    "        if lid != 'out':\n",
    "          h = tf.nn.relu(tf.matmul(h,w)+b,name=lid+'_output')\n",
    "        else:\n",
    "          h = tf.nn.xw_plus_b(h,w,b,name=lid+'_output')\n",
    "\n",
    "tf_predictions = tf.nn.softmax(h, name='predictions')\n",
    "# Calculating Loss\n",
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=train_labels, logits=h),name='loss')\n",
    "\n",
    "# Optimizer \n",
    "tf_learning_rate = tf.placeholder(tf.float32, shape=None, name='learning_rate')\n",
    "optimizer = tf.train.MomentumOptimizer(tf_learning_rate,momentum=0.9)\n",
    "grads_and_vars = optimizer.compute_gradients(tf_loss)\n",
    "tf_loss_minimize = optimizer.minimize(tf_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tensorboard Summaries\n",
    "\n",
    "Here you can define the `tf.summary` objects. `tf.summary` objects are the type of entities understood by the Tensorboard. This means that whatever value you'd like to be displayed on the Tensorboard, you should encapsulate it as a `tf.summary` object. There are several different types of summaries. Here as you are visualizing only scalars, you can define `tf.summary.scalar` objects. Furthermore, you can use `tf.name_scope` to group scalars on the Tensorboard. That is, scalrs having the same name scope will be displayed on the same row on the Tensorboard. Here you define three different summaries.\n",
    "\n",
    "* `tf_loss_summary` : You feed in a value by means of a placeholder, whenever you need to publish this to the Tensorboard\n",
    "* `tf_accuracy_summary` : You feed in a value by means of a placeholder, whenever you need to publish this to the Tensorboard\n",
    "* `tf_gradnorm_summary` : This calculates the l2 norm of the gradients of the last layer of your neural network. Gradient norm is a good indicator of whether the weights of the neural network are being properly updated. A too small gradient norm can indicate *vanishing gradient* or a too large gradient can imply *exploding gradient* phenomenon.\n",
    "\n",
    "**Note**: This is identical to the code you used first time you defined these operations and tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name scope allows you to group various summaries together\n",
    "# Summaries having the same name_scope will be displayed on the same row on the Tensorboard\n",
    "with tf.name_scope('performance'):\n",
    "    # Summaries need to display on the Tensorboard\n",
    "    # Whenever you need to record the loss, feed the mean loss to this placeholder\n",
    "    tf_loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary') \n",
    "    # Create a scalar summary object for the loss so Tensorboard knows how to display it\n",
    "    tf_loss_summary = tf.summary.scalar('loss', tf_loss_ph)\n",
    "\n",
    "    # Whenever you need to record the loss, feed the mean test accuracy to this placeholder\n",
    "    tf_accuracy_ph = tf.placeholder(tf.float32,shape=None, name='accuracy_summary') \n",
    "    # Create a scalar summary object for the accuracy so Tensorboard knows how to display it\n",
    "    tf_accuracy_summary = tf.summary.scalar('accuracy', tf_accuracy_ph)\n",
    "\n",
    "# Gradient norm summary\n",
    "for g,v in grads_and_vars:\n",
    "    if 'hidden5' in v.name and 'weights' in v.name:\n",
    "        with tf.name_scope('gradients'):\n",
    "            tf_last_grad_norm = tf.sqrt(tf.reduce_mean(g**2))\n",
    "            tf_gradnorm_summary = tf.summary.scalar('grad_norm', tf_last_grad_norm)\n",
    "            break\n",
    "# Merge all summaries together\n",
    "performance_summaries = tf.summary.merge([tf_loss_summary,tf_accuracy_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Histogram Summaries to Visualize Weights and Biases \n",
    "\n",
    "Here you again define the `tf.summary` objects. However now you are visualizing vectors of scalars so you need to define `tf.summary.histogram` objects. In this case, you define two histogram objects (namely, `tf_w_hist` and `tf_b_hist`) that contains weights and biases of agiven layer. You will define such histogram objects for all the layers and each layer will have it's own name scope. Finally you can use the `tf.summary.merge` operation to create a grouped operation that execute all these summaries at once.\n",
    "\n",
    "**Note**: This is identical to the code you used first time you defined these operations and tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summaries need to display on the Tensorboard\n",
    "# Create a summary for each weight bias in each layer\n",
    "all_summaries = []\n",
    "for lid in layer_ids:\n",
    "    with tf.name_scope(lid+'_hist'):\n",
    "        with tf.variable_scope(lid,reuse=True):\n",
    "            w,b = tf.get_variable('weights'), tf.get_variable('bias')\n",
    "\n",
    "            # Create a scalar summary object for the loss so Tensorboard knows how to display it\n",
    "            tf_w_hist = tf.summary.histogram('weights_hist', tf.reshape(w,[-1]))\n",
    "            tf_b_hist = tf.summary.histogram('bias_hist', b)\n",
    "            all_summaries.extend([tf_w_hist, tf_b_hist])\n",
    "\n",
    "# Merge all parameter histogram summaries together\n",
    "tf_param_summaries = tf.summary.merge(all_summaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the neural network model\n",
    "\n",
    "In the code below you do the following. First you create a session, in which you execute the operations you defined above. Then you create folder for saving summary data. You next create a summary write `summ_writer`. You can now initialize all variables. This will be followed by loading the MNIST dataset.\n",
    "\n",
    "Then for each epoch, and each batch in training data (that is, each iteration). Execute `gradnorm_summary` and `tf_param_summaries` if it is the first iteration and write `gradnorm_summary` and `tf_param_summaries` to event file with summary writer. You now execute model optimization and calculating the loss. After you go through the full training dataset for a single epoch, calculate average training loss.\n",
    "\n",
    "You follow a similar treatment for the validation dataset as well. Specifically, for each batch in validation data, you calculate validation accuracy for each batch. Thereafter calculate average validation accuracy for full validation set.\n",
    "\n",
    "Finally, the testing phase is executed. In this, for each batch in test data, you calculate test accuracy for each batch. With that, you calculate average test accuracy for full test set. At the very end you execute `performance_summaries` and write them to event file with the summary writer.\n",
    "\n",
    "**Note**: This is as same as you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss in epoch 0: 0.44695\n",
      "\tAverage Valid Accuracy in epoch 0: 95.92000\n",
      "\tAverage Test Accuracy in epoch 0: 95.50000\n",
      "\n",
      "Average loss in epoch 1: 0.13685\n",
      "\tAverage Valid Accuracy in epoch 1: 96.78000\n",
      "\tAverage Test Accuracy in epoch 1: 96.28000\n",
      "\n",
      "Average loss in epoch 2: 0.08945\n",
      "\tAverage Valid Accuracy in epoch 2: 97.18000\n",
      "\tAverage Test Accuracy in epoch 2: 97.14000\n",
      "\n",
      "Average loss in epoch 3: 0.06410\n",
      "\tAverage Valid Accuracy in epoch 3: 97.54000\n",
      "\tAverage Test Accuracy in epoch 3: 97.56000\n",
      "\n",
      "Average loss in epoch 4: 0.04689\n",
      "\tAverage Valid Accuracy in epoch 4: 98.06000\n",
      "\tAverage Test Accuracy in epoch 4: 97.75000\n",
      "\n",
      "Average loss in epoch 5: 0.03310\n",
      "\tAverage Valid Accuracy in epoch 5: 97.98000\n",
      "\tAverage Test Accuracy in epoch 5: 97.83000\n",
      "\n",
      "Average loss in epoch 6: 0.02627\n",
      "\tAverage Valid Accuracy in epoch 6: 97.96000\n",
      "\tAverage Test Accuracy in epoch 6: 97.72000\n",
      "\n",
      "Average loss in epoch 7: 0.02006\n",
      "\tAverage Valid Accuracy in epoch 7: 98.10000\n",
      "\tAverage Test Accuracy in epoch 7: 97.96000\n",
      "\n",
      "Average loss in epoch 8: 0.01436\n",
      "\tAverage Valid Accuracy in epoch 8: 98.34000\n",
      "\tAverage Test Accuracy in epoch 8: 98.20000\n",
      "\n",
      "Average loss in epoch 9: 0.00902\n",
      "\tAverage Valid Accuracy in epoch 9: 98.26000\n",
      "\tAverage Test Accuracy in epoch 9: 98.04000\n",
      "\n",
      "Average loss in epoch 10: 0.00522\n",
      "\tAverage Valid Accuracy in epoch 10: 98.36000\n",
      "\tAverage Test Accuracy in epoch 10: 98.20000\n",
      "\n",
      "Average loss in epoch 11: 0.00262\n",
      "\tAverage Valid Accuracy in epoch 11: 98.40000\n",
      "\tAverage Test Accuracy in epoch 11: 98.27000\n",
      "\n",
      "Average loss in epoch 12: 0.00252\n",
      "\tAverage Valid Accuracy in epoch 12: 98.46000\n",
      "\tAverage Test Accuracy in epoch 12: 98.26000\n",
      "\n",
      "Average loss in epoch 13: 0.00184\n",
      "\tAverage Valid Accuracy in epoch 13: 98.52000\n",
      "\tAverage Test Accuracy in epoch 13: 98.36000\n",
      "\n",
      "Average loss in epoch 14: 0.00059\n",
      "\tAverage Valid Accuracy in epoch 14: 98.46000\n",
      "\tAverage Test Accuracy in epoch 14: 98.22000\n",
      "\n",
      "Average loss in epoch 15: 0.00048\n",
      "\tAverage Valid Accuracy in epoch 15: 98.62000\n",
      "\tAverage Test Accuracy in epoch 15: 98.43000\n",
      "\n",
      "Average loss in epoch 16: 0.00038\n",
      "\tAverage Valid Accuracy in epoch 16: 98.58000\n",
      "\tAverage Test Accuracy in epoch 16: 98.38000\n",
      "\n",
      "Average loss in epoch 17: 0.00032\n",
      "\tAverage Valid Accuracy in epoch 17: 98.60000\n",
      "\tAverage Test Accuracy in epoch 17: 98.41000\n",
      "\n",
      "Average loss in epoch 18: 0.00024\n",
      "\tAverage Valid Accuracy in epoch 18: 98.58000\n",
      "\tAverage Test Accuracy in epoch 18: 98.40000\n",
      "\n",
      "Average loss in epoch 19: 0.00019\n",
      "\tAverage Valid Accuracy in epoch 19: 98.56000\n",
      "\tAverage Test Accuracy in epoch 19: 98.43000\n",
      "\n",
      "Average loss in epoch 20: 0.00016\n",
      "\tAverage Valid Accuracy in epoch 20: 98.56000\n",
      "\tAverage Test Accuracy in epoch 20: 98.43000\n",
      "\n",
      "Average loss in epoch 21: 0.00014\n",
      "\tAverage Valid Accuracy in epoch 21: 98.56000\n",
      "\tAverage Test Accuracy in epoch 21: 98.41000\n",
      "\n",
      "Average loss in epoch 22: 0.00013\n",
      "\tAverage Valid Accuracy in epoch 22: 98.56000\n",
      "\tAverage Test Accuracy in epoch 22: 98.44000\n",
      "\n",
      "Average loss in epoch 23: 0.00012\n",
      "\tAverage Valid Accuracy in epoch 23: 98.56000\n",
      "\tAverage Test Accuracy in epoch 23: 98.42000\n",
      "\n",
      "Average loss in epoch 24: 0.00011\n",
      "\tAverage Valid Accuracy in epoch 24: 98.58000\n",
      "\tAverage Test Accuracy in epoch 24: 98.40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_size = 28\n",
    "n_channels = 1\n",
    "n_classes = 10\n",
    "n_train = 55000\n",
    "n_valid = 5000\n",
    "n_test = 10000\n",
    "n_epochs = 25\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9 # making sure Tensorflow doesn't overflow the GPU\n",
    "\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "if not os.path.exists('summaries'):\n",
    "    os.mkdir('summaries')\n",
    "if not os.path.exists(os.path.join('summaries','fourth')):\n",
    "    os.mkdir(os.path.join('summaries','fourth'))\n",
    "    \n",
    "summ_writer_4 = tf.summary.FileWriter(os.path.join('summaries','fourth'), session.graph)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "accuracy_per_epoch = []\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_per_epoch = []\n",
    "    for i in range(n_train//batch_size):\n",
    "        \n",
    "        # =================================== Training for one step ========================================\n",
    "        batch = mnist_data.train.next_batch(batch_size)    # Get one batch of training data\n",
    "        if i == 0:\n",
    "            # Only for the first epoch, get the summary data\n",
    "            # Otherwise, it can clutter the visualization\n",
    "            l,_,gn_summ, wb_summ = session.run([tf_loss,tf_loss_minimize,tf_gradnorm_summary, tf_param_summaries],\n",
    "                                      feed_dict={train_inputs: batch[0].reshape(batch_size,image_size*image_size),\n",
    "                                                 train_labels: batch[1],\n",
    "                                                tf_learning_rate: 0.01})\n",
    "            summ_writer_4.add_summary(gn_summ, epoch)\n",
    "            summ_writer_4.add_summary(wb_summ, epoch)\n",
    "        else:\n",
    "            # Optimize with training data\n",
    "            l,_ = session.run([tf_loss,tf_loss_minimize],\n",
    "                              feed_dict={train_inputs: batch[0].reshape(batch_size,image_size*image_size),\n",
    "                                         train_labels: batch[1],\n",
    "                                         tf_learning_rate: 0.01})\n",
    "        loss_per_epoch.append(l)\n",
    "        \n",
    "    print('Average loss in epoch %d: %.5f'%(epoch,np.mean(loss_per_epoch)))    \n",
    "    avg_loss = np.mean(loss_per_epoch)\n",
    "    \n",
    "    # ====================== Calculate the Validation Accuracy ==========================\n",
    "    valid_accuracy_per_epoch = []\n",
    "    for i in range(n_valid//batch_size):\n",
    "        valid_images,valid_labels = mnist_data.validation.next_batch(batch_size)\n",
    "        valid_batch_predictions = session.run(\n",
    "            tf_predictions,feed_dict={train_inputs: valid_images.reshape(batch_size,image_size*image_size)})\n",
    "        valid_accuracy_per_epoch.append(accuracy(valid_batch_predictions,valid_labels))\n",
    "        \n",
    "    mean_v_acc = np.mean(valid_accuracy_per_epoch)\n",
    "    print('\\tAverage Valid Accuracy in epoch %d: %.5f'%(epoch,np.mean(valid_accuracy_per_epoch)))\n",
    "    \n",
    "    # ===================== Calculate the Test Accuracy ===============================\n",
    "    accuracy_per_epoch = []\n",
    "    for i in range(n_test//batch_size):\n",
    "        test_images, test_labels = mnist_data.test.next_batch(batch_size)\n",
    "        test_batch_predictions = session.run(\n",
    "            tf_predictions,feed_dict={train_inputs: test_images.reshape(batch_size,image_size*image_size)}\n",
    "        )\n",
    "        accuracy_per_epoch.append(accuracy(test_batch_predictions,test_labels))\n",
    "        \n",
    "    print('\\tAverage Test Accuracy in epoch %d: %.5f\\n'%(epoch,np.mean(accuracy_per_epoch)))\n",
    "    avg_test_accuracy = np.mean(accuracy_per_epoch)\n",
    "    \n",
    "    # Execute the summaries defined above\n",
    "    summ = session.run(performance_summaries, feed_dict={tf_loss_ph:avg_loss, tf_accuracy_ph:avg_test_accuracy})\n",
    "\n",
    "    # Write the obtained summaries to the file, so it can be displayed in the Tensorboard\n",
    "    summ_writer_4.add_summary(summ, epoch)\n",
    "    \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different Initialization Techniques\n",
    "\n",
    "Here you compare how weights evolve over time for the two different initalizations; *truncated_normal_initializer* (red) and *xavier_initializer* (blue). You can see that xavier initializer keeps more weights away from zero than the normal initializer, which is a better thing to do. This is potentially allowing the xavier initialized neural networks to converge faster, as evident by the loss/accuracy curves. \n",
    "\n",
    "<img src=\"tensorboard_3_and_4.jpg\" style=\"height: 600px; float:left;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distibution View of Histograms\n",
    "\n",
    "You now can compare the difference between the two views; histogram view and the distribution view. Distribution view is essentiall a different way of looking at the histograms. If you look at the image below, you can easily see that the distribution view is a top view of the histogram view (I have rotated the histogram graphs to easily see the resemblance).\n",
    "\n",
    "<img src=\"tensorboard_histogram_vs_distribution_views.png\" style=\"height: 400px; float:left;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial you saw how to use the Tensorboard. First you learnt how to start the Tensorboard through the command prompt (Windows) or terminal (Ubuntu/Mac). Next you looked at different views of data provided by the Tensorboard. You then looked at code that visualizes scalar values (e.g. loss / accuracy). You used a feed-forward neural network model to concretely understand the use of the scalar value visualization. Thereafter, you explored how you can visualize collections/vectors of scalars using the histogram view. This was followed by a comparison highlighting the differences between neural network weight initialization techniques using the histogram view. Finally you discussed the similarities between the distribution view and the histogram view.\n",
    "\n",
    "* Author: Thushan Ganegedara\n",
    "* Email: thushv@gmail.com\n",
    "* Website: http://www.thushv.com/\n",
    "* LinkedIn: https://www.linkedin.com/in/thushanganegedara/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
